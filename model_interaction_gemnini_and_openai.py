# -*- coding: utf-8 -*-
"""Model_Interaction_Gemnini and OpenAI Day1 b.ipynb

Automatically generated by Colab.


## Frontier Model APIs

APIs of Google and OpenAI.

## Setting up your keys

If you haven't done so already, you could now create API keys for Google and OpenAI.

For OpenAI, visit https://openai.com/api/  
For Google, visit https://ai.google.dev/gemini-api  


### Adding API keys to your .env file

When you get your API keys, you need to set them as in the secrets

```
OPENAI_API_KEY=xxxx
GOOGLE_API_KEY=xxxx
```

Afterwards, you may need to restart your google colab
"""

# imports

import os
from google.colab import userdata
from openai import OpenAI
from IPython.display import Markdown, display, update_display
import google.generativeai

# Load environment variables in a file called .env
# Print the key prefixes to help with any debugging

openai_api_key = userdata.get('openai key')
google_api_key = userdata.get('gemini key')

if openai_api_key:
    print(f"OpenAI API Key exists and begins {openai_api_key[:8]}")
else:
    print("OpenAI API Key not set")

if google_api_key:
    print(f"Google API Key exists and begins {google_api_key[:8]}")
else:
    print("Google API Key not set")

openai =  OpenAI(api_key=openai_api_key)
google.generativeai.configure()

"""## Asking LLMs to tell a joke

It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.
Later we will be putting LLMs to better use!

### What information is included in the API

Typically we'll pass to the API:
- The name of the model that should be used
- A system message that gives overall context for the role the LLM is playing
- A user message that provides the actual prompt

There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.
"""

system_message = "You are an assistant that is great at telling jokes"
user_prompt = "Tell a light-hearted joke for an audience of Data Scientists"

prompts = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]



# GPT-4o-mini

completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)
print(completion.choices[0].message.content)



# GPT-4.1-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4.1-mini',
    messages=prompts,
    temperature=0.7
)
print(completion.choices[0].message.content)

"""# **What Is the temperature Parameter?**

temperature controls how random or deterministic the model’s responses are.
It adjusts the “creativity level” or “risk-taking” in word choice when generating text.


**How It Works**
When the model predicts the next word, it doesn’t choose a single word — it assigns probabilities to many possible next words.

A low temperature (close to 0) → the model picks the highest-probability words almost every time → factual, stable, consistent output.

A high temperature (closer to 1) → the model samples from a wider range of possibilities → creative, variable, and sometimes surprising output.
"""

# GPT-4.1-nano - extremely fast and cheap

completion = openai.chat.completions.create(
    model='gpt-4.1-nano',
    messages=prompts
)
print(completion.choices[0].message.content)

# GPT-4.1

completion = openai.chat.completions.create(
    model='gpt-4.1',
    messages=prompts,
    temperature=0.4
)
print(completion.choices[0].message.content)

# If you have access to this, here is the reasoning model o4-mini
# This is trained to think through its response before replying
# So it will take longer but the answer should be more reasoned - not that this helps..

completion = openai.chat.completions.create(
    model='o4-mini',
    messages=prompts
)
print(completion.choices[0].message.content)

completion = openai.chat.completions.create(
    model="gemini-2.5-flash",
    messages=prompts
)
print(completion.choices[0].message.content)

gemini_via_openai_client = OpenAI(
    api_key=google_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = gemini_via_openai_client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=prompts
)
print(response.choices[0].message.content)

"""# Sidenote:

This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.

So much so, that all the models now support this approach - including Anthropic.

You can read more about this approach, with 4 examples, in the first section of this guide:

https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb

## Back to OpenAI with a serious question
"""

# To be serious! GPT-4o-mini with the original question

prompts = [
    {"role": "system", "content": "You are a helpful assistant that responds in Markdown"},
    {"role": "user", "content": "How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown."}
  ]

# Have it stream back results in markdown

stream = openai.chat.completions.create(
    model='gpt-4.1-mini',
    messages=prompts,
    temperature=0.7,
    stream=True
)

reply = ""
display_handle = display(Markdown(""), display_id=True)
for chunk in stream:
    reply += chunk.choices[0].delta.content or ''
    reply = reply.replace("```","").replace("markdown","")
    update_display(Markdown(reply), display_id=display_handle.display_id)

"""Code Explanation: https://docs.google.com/document/d/1EtURr0Yw8oa9iOtNYZL9Qro51NkKrKPi5flnxXfMO_4/edit?usp=sharing

## And now for some fun - an adversarial conversation between Chatbots..

You're already familar with prompts being organized into lists like:

```
[
    {"role": "system", "content": "system message here"},
    {"role": "user", "content": "user prompt here"}
]
```

In fact this structure can be used to reflect a longer conversation history:

```
[
    {"role": "system", "content": "system message here"},
    {"role": "user", "content": "first user prompt here"},
    {"role": "assistant", "content": "the assistant's response"},
    {"role": "user", "content": "the new user prompt"},
]
```

And we can use this approach to engage in a longer interaction with history.

The assistant role in LLM messages is where the large language model (LLM) generates its response to the user. It represents the AI's turn in a conversation, providing answers, explanations, or completing tasks based on the user's input and any system instructions.

**What is the role of assistant in LLM?**
The assistant message can include answers, explanations, suggestions, or any other form of response that the chat model is capable of generating. In a chat interaction, the assistant message continues the flow of conversation by providing information or addressing the user's queries.


**How the assistant role functions**

-It's the AI's response: When a user sends a "user" message (their question or command), the LLM processes it and generates a reply. This reply is designated as an "assistant" message.

-It maintains context: In multi-turn conversations, the assistant role is crucial for keeping track of the dialogue history, enabling the LLM to provide coherent and context-aware responses.

-It delivers information: The assistant's message can contain answers, suggestions, code, or other content the LLM is capable of generating.

-It follows system instructions: The assistant's output is guided by the "system" message, which provides guidelines on the LLM's behavior, persona, and overall purpose.


Example of roles in a conversation
-System: "You are a helpful AI assistant that answers questions about history."
-User: "Who was the first president of the United States?"
-Assistant: "The first president of the United States was George Washington."
"""



# Let's make a conversation between GPT-4.1-mini and Gemini
# We're using cheap versions of models so the costs will be minimal

gpt_model = "gpt-4.1-mini"
gemini_model = "gemini-2.5-flash"

gpt_system = "You are a chatbot who is very argumentative; \
you disagree with anything in the conversation and you challenge everything, in a snarky way."

gemini_system = "You are a very polite, courteous chatbot. You try to agree with \
everything the other person says, or find common ground. If the other person is argumentative, \
you try to calm them down and keep chatting."

gpt_messages = ["Hi there"]
gemini_messages = ["Hi"]

def call_gpt():
    messages = [{"role": "system", "content": gpt_system}]
    for gpt, gemini in zip(gpt_messages, gemini_messages):
        messages.append({"role": "assistant", "content": gpt})
        messages.append({"role": "user", "content": gemini})
    completion = openai.chat.completions.create(
        model=gpt_model,
        messages=messages
    )
    return completion.choices[0].message.content

call_gpt()

def call_gemini():
    messages = [{"role": "system", "content": gemini_system}]

    for gpt, gemini in zip(gpt_messages, gemini_messages):
        messages.append({"role": "user", "content": gpt})         # GPT is the user
        messages.append({"role": "assistant", "content": gemini}) # Gemini's past reply

    # Get Gemini's next reply to GPT's last message
    messages.append({"role": "user", "content": gpt_messages[-1]})

    response = gemini_via_openai_client.chat.completions.create(
        model=gemini_model,
        messages=messages,
        max_tokens=500
    )

    return response.choices[0].message.content

call_gemini()

system_message = "explains complex artificial intelligence concepts to beginners in the simplest, most visual way possible."
user_prompt = "explain Neural networks"
prompts = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]

  # GPT-4o-mini

completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)
print(completion.choices[0].message.content)

system_message = ''' You are a structured AI trainer. When users
give you examples, learn the pattern and
continue it in the same tone, style, and
structure. Always keep consistency in how
you explain AI concepts, and mimic the
example format precisely'''

user_prompt = "explain Neural networks"
prompts = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]

  # GPT-4o-mini

completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)
print(completion.choices[0].message.content)

system_message ='''Role
 Act as a Generative AI Learning Assistant who teaches students complex GenAI and AI concepts in an easy-to-understand yet technically accurate way. You communicate like an engaging instructor — clear, friendly, and confident. Your explanations always balance technical depth with relatable analogies.
Task
Begin with a short overview of the concept in simple language (3–4 sentences).


Explain the key technical idea behind it with correct terminology but simplified structure.


Provide at least one real-world example or analogy to make it relatable.


Where relevant, contrast it with older AI approaches (e.g., rule-based systems vs LLMs).


End with 2–3 practical use cases where the concept applies in business or daily life.


Context
The audience is students and professionals new to Generative AI, with limited coding experience.


The goal is to make them job-ready by building both conceptual understanding and applied intuition.


Explanations should assume minimal prior knowledge but maintain correctness.


Use examples relevant to current GenAI tools (OpenAI, Gemini, Claude, Mistral, Falcon, and Humane LLM).


Keep tone conversational, encouraging, and inclusive.


Reasoning
Before answering, outline your internal reasoning:


What is the essence of this concept?


What’s the simplest accurate way to explain it?


Which analogy would best help a beginner connect to it?


Then deliver the refined, structured response.


Output Format
 Return your final answer in this structure in markdown:
### Concept Overview
[Explain simply]

### Technical Insight
[Provide core idea with light technical depth]

### Example or Analogy
[Relatable real-world example]

### Real-World Applications
[2–3 bullet points showing where it’s used]

Stop Conditions
Stop once the explanation covers the concept, its example, and its applications clearly.


Do not generate extra definitions or unrelated facts.


End when the learner could re-explain the idea confidently in their own words.

'''

user_prompt = "explain Neural networks"
prompts = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]

  # GPT-4o-mini

completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)
print(completion.choices[0].message.content)

